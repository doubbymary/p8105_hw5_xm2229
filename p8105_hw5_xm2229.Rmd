---
title: "p8105_hw5_xm2229"
author: "Xiaoyue Ma"
date: "11/9/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 1

```{r}
library(tidyverse)

set.seed(10)

iris_with_missing = iris %>%
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(Species = as.character(Species))
```

There are two cases to address:

 * For numeric variables, you should fill in missing values with the mean of non-missing values
 * For character variables, you should fill in missing values with `virginica`
 
Write a function that takes a vector as an argument; replaces missing values using the rules defined above; and returns the resulting vector.

```{r}
rule_rep = function(vector) {
  type = typeof(vector)
  if (type == "character") {
    new_v = replace_na(vector, "virginica")
  } else {
    new_v = replace_na(vector, mean(vector, na.rm = TRUE))
  }
  return(new_v)
}
```


Apply this function to the columns of `iris_with_missing` using a `map` statement. The table printed below is the first 22 lines of the columns of `iris_with_missing` after applying the function and we can see that indeed, all `NA` values has been replaced strictlly following the rules provided above.

```{r}
iris_with_missing %>%
  map_df(~rule_rep(.x)) %>%
  head(22) %>%
  knitr::kable()
```


## Problem 2


Create a tidy dataframe containing data from all participants, including the subject ID, arm, and observations over time:

 * Start with a dataframe containing all file names; the `list.files` function will help
 * Iterate over file names and read in data for each subject using `purrr::map` and saving the result as a new variable in the dataframe
 * Tidy the result; manipulate file names to include control arm and subject ID, make sure weekly observations are “tidy”, and do any other tidying that’s necessary


```{r, message=FALSE}
path_to_dir = "./data"

## a helper function for 
## 1. concatenating path to file
## 2. read the file as dataframe
## 3. append arm and subject ID
read_files = function(x){
  # concatenate the path to directory with file name
  path_to_file = paste(path_to_dir, x, sep = "/")
  # read in the data as a dataframe
  row = read_csv(path_to_file)
  # extract string from file name
  splited_file = unlist(strsplit(x, ".", fixed = TRUE))
  # extract arm group and id
  arm_id = unlist(strsplit(splited_file[1], "_", fixed = TRUE))
  # append arm group and id
  row$arm = arm_id[1]
  row$id = as.integer(arm_id[2])
  return(row)
}

arm = purrr::map(list.files(path_to_dir), read_files) %>%
  bind_rows() %>%
  pivot_longer(cols = starts_with("week"), 
               names_to = "week",
               names_prefix = "week_",
               values_to = "data") %>%
  mutate(week = as.integer(week),
         arm = forcats::fct_relevel(arm, "con", "exp")) %>%
  janitor::clean_names()

```

The tidyed data is called `arm`. It contains `r ncol(arm)` columns and `r nrow(arm)` observations. The columns are:

 * `arm`: a factor column that contains whether if a sample is control group ( __con__ ) or experiemental group ( __exp__ )
 * `id`: contains the id of the sample
 * `week`: contains the number of week the observation belong, which is an integer column
 * `data`: contains all the data measured in the longitudinal study 

Below is a spaghetti plot showing observations on each subject over time. According to the plot, there is clearly a trend that the experimental group has a signifcant growth as time past and the control group stays the same. 

```{r, fig.width = 12}
arm %>% 
  ggplot(aes(x = week, y = data, group = arm, color = arm)) +
  geom_path() + 
  labs(title = "Data measured on control group and experimental group by week", colour = "ARM groups") + 
  ylab("Measured Data") + 
  xlab("Number of Week")
```

## Problem 3

Below is a function that generate 1 dataset according to the design elements. And the table below is first 6 rows of a sample dataset generated by the function.

```{r}
generate_dataset = function(n=30, beta_0=2, var=50, beta_1=0){
  dataset = data.frame(y = rep(0,n), x = rep(0,n))
  for (j in 1:n){
    # draw xi1
    dataset[j, 2] = rnorm(1)
    # draw error
    error = rnorm(1, mean = 0, sd = sqrt(var))
    # calculate yi
    dataset[j, 1] = beta_0 + beta_1 * dataset[j, 2] + error
  }
  return(dataset)
}

head(generate_dataset(beta_1 = 1)) %>% knitr::kable()
```

A function that calculate $\hat{\beta_1}$ and p-value for __N__ generated dataset given the value of number of dataset wanted to generate and the value of $\beta_1$. Table below are the fist 6 rows of the values of $\hat{\beta_1}$ and their corresponding p-value for the 10000 generated dataset, when $\beta_1 = 0$.

```{r}
result_cal = function(N= 10000, b_1 = 0){
  result = data.frame(estimator = rep(0, N), p_value = rep(0,N))
  for (i in 1:N){
    ds = generate_dataset(beta_1 = b_1)
    output = lm(y~x, data = ds) %>%
      broom::tidy() %>%
      select(estimate, p.value)
    result[i,1] = output[2,1]
    result[i,2] = output[2,2]
  }
  return(result)
}

zero = result_cal()
head(zero) %>% knitr::kable()
```

Repeat the above for $\beta_1=\{1,2,3,4,5,6\}$ compute the estimators and the p-values for 10000. All the data is stored in one dataframe called `simulation`

```{r}
## helper function to generate result for each dataset
## and append the beta value in the last column
build_ds = function(x){
  est = result_cal(b_1 = x)
  est$beta = x
  return(est)
}

simulation = purrr::map(1:6, build_ds) %>%
  bind_rows()
```


Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of $\beta_1$ on the x axis. From the graph below, we can see that as the effect size grow, more null hypothesis got rejected, which means more estimated beta is significant.

```{r}
alpha = 0.05

simulation %>%
  mutate(power = ifelse(p_value >= alpha, 0, 1)) %>%
  group_by(beta) %>%
  summarise(proportion = sum(power)/n()) %>%
  ggplot(aes(x= beta, y = proportion)) +
  geom_path() + ylim(0, 1.2) + xlab("Effect Size") + ylab("Proportion of null rejected") + 
  scale_x_discrete(limits=c("1","2","3","4","5","6")) +
  geom_text(aes(label=proportion), hjust = 0.5, vjust = -2) + 
  labs(title = "Proportion of null rejected by effect size")
```


Make a plot showing the average estimate of $\hat{\beta_1}$ on the y axis and the true value of $\beta_1$ on the x axis. From this graph, we can say that there is a perfect linear relationship between the two. As the true value increases, the average of the estimators will also increase.


```{r}
plot1 = simulation %>%
  group_by(beta) %>%
  summarise(average = mean(estimator)) %>%
  ggplot(aes(x= beta, y = average, color = "darkblue")) +
  geom_path() + 
  scale_x_discrete(limits=c("1","2","3","4","5","6")) +
  geom_text(aes(label=round(average,2)), hjust = 0.5, vjust = 2) + ylim(0,7)
  
  
plot1 + labs(title = "Average of the estimators against True values")+ xlab("True values") + ylab("Average of the estimators") +
  theme(legend.position = "none")
```

Make a second plot (or overlay on the first) the average estimate of $\hat{\beta_1}$ only in samples for which the null was rejected on the y axis and the true value of $\beta_1$ on the x axis. According to the graph below, the sample average of $\hat{\beta_1}$ across tests for which the null is rejected approximately equal to the true value of $\beta_1$ when the effect size is large. When the true value of $\beta_1$ is small, there is a significant difference. This is becuase that as the effect size grow larger the power of the test is stronger. Therefore, the test will be less like to be wrong when the true value of $\beta$ is further away from zero on number line.


```{r}
filtered_avearge <- simulation %>%
  mutate(power = ifelse(p_value >= alpha, 0, 1)) %>%
  filter(power == 1) %>%
  group_by(beta)%>%
  summarise(average = mean(estimator))

plot1 + geom_path(data=filtered_avearge, aes(color="red")) + 
  geom_text(data=filtered_avearge,
            aes(label = round(average, 2), color = "red"),
            vjust = -2) + 
  scale_color_discrete(name = "", labels = c("Not Filtered", "Filtered")) + 
  labs(title = "Average of the estimators against True values") + 
  ylab("Average of the estimators") + xlab("True values")
```






